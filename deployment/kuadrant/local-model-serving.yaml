# Local model serving configurations for minikube
# These use hostPath storage instead of S3 for simplicity

---
# Simple HTTP model server for testing (replaces KServe for now)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: granite-model-server
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: granite-model-server
  template:
    metadata:
      labels:
        app: granite-model-server
        app.kubernetes.io/name: kserve-container
    spec:
      containers:
      - name: model-server
        image: python:3.9-slim
        command: ["/bin/bash"]
        args:
          - -c
          - |
            pip install flask
            cat > /app.py << 'EOF'
            from flask import Flask, request, jsonify
            import json
            import random
            import time
            
            app = Flask(__name__)
            
            @app.route('/health', methods=['GET'])
            def health():
                return jsonify({"status": "healthy"})
            
            @app.route('/v1/chat/completions', methods=['POST'])
            def chat_completions():
                data = request.json
                
                # Simulate processing time
                time.sleep(0.5)
                
                # Mock response
                response = {
                    "id": f"chatcmpl-{random.randint(1000, 9999)}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": "granite-8b-code-instruct-128k",
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"Hello! This is a mock response from Granite model. You asked: {data.get('messages', [{}])[-1].get('content', 'nothing')}"
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": random.randint(10, 50),
                        "completion_tokens": random.randint(10, 100),
                        "total_tokens": random.randint(20, 150)
                    }
                }
                return jsonify(response)
            
            if __name__ == '__main__':
                app.run(host='0.0.0.0', port=8080)
            EOF
            python /app.py
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: granite-8b-code-instruct-128k-predictor
  namespace: llm
spec:
  selector:
    app: granite-model-server
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
---
# Mistral model server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mistral-model-server
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mistral-model-server
  template:
    metadata:
      labels:
        app: mistral-model-server
        app.kubernetes.io/name: kserve-container
    spec:
      containers:
      - name: model-server
        image: python:3.9-slim
        command: ["/bin/bash"]
        args:
          - -c
          - |
            pip install flask
            cat > /app.py << 'EOF'
            from flask import Flask, request, jsonify
            import json
            import random
            import time
            
            app = Flask(__name__)
            
            @app.route('/health', methods=['GET'])
            def health():
                return jsonify({"status": "healthy"})
            
            @app.route('/v1/chat/completions', methods=['POST'])
            def chat_completions():
                data = request.json
                
                # Simulate processing time
                time.sleep(0.3)
                
                # Mock response
                response = {
                    "id": f"chatcmpl-{random.randint(1000, 9999)}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": "mistral-7b-instruct",
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"Bonjour! This is a mock response from Mistral model. You asked: {data.get('messages', [{}])[-1].get('content', 'nothing')}"
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": random.randint(5, 30),
                        "completion_tokens": random.randint(5, 80),
                        "total_tokens": random.randint(10, 110)
                    }
                }
                return jsonify(response)
            
            if __name__ == '__main__':
                app.run(host='0.0.0.0', port=8080)
            EOF
            python /app.py
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: mistral-7b-instruct-predictor
  namespace: llm
spec:
  selector:
    app: mistral-model-server
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
---
# Nomic embeddings server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nomic-model-server
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nomic-model-server
  template:
    metadata:
      labels:
        app: nomic-model-server
        app.kubernetes.io/name: kserve-container
    spec:
      containers:
      - name: model-server
        image: python:3.9-slim
        command: ["/bin/bash"]
        args:
          - -c
          - |
            pip install flask numpy
            cat > /app.py << 'EOF'
            from flask import Flask, request, jsonify
            import json
            import random
            import time
            import numpy as np
            
            app = Flask(__name__)
            
            @app.route('/health', methods=['GET'])
            def health():
                return jsonify({"status": "healthy"})
            
            @app.route('/embeddings', methods=['POST'])
            def embeddings():
                data = request.json
                
                # Simulate processing time
                time.sleep(0.2)
                
                # Generate mock embeddings
                input_text = data.get('input', '')
                if isinstance(input_text, list):
                    embeddings = [np.random.randn(768).tolist() for _ in input_text]
                else:
                    embeddings = [np.random.randn(768).tolist()]
                
                response = {
                    "object": "list",
                    "data": [
                        {
                            "object": "embedding",
                            "index": i,
                            "embedding": emb
                        } for i, emb in enumerate(embeddings)
                    ],
                    "model": "nomic-embed-text-v1.5",
                    "usage": {
                        "prompt_tokens": len(str(input_text).split()),
                        "total_tokens": len(str(input_text).split())
                    }
                }
                return jsonify(response)
            
            if __name__ == '__main__':
                app.run(host='0.0.0.0', port=8080)
            EOF
            python /app.py
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: nomic-embed-text-v1-5-predictor
  namespace: llm
spec:
  selector:
    app: nomic-model-server
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP