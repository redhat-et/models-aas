# vLLM Simulator as KServe InferenceService
# Uses KServe InferenceService for consistency with real models
# This bypasses the need for a custom ServingRuntime by using a generic container runtime
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: vllm-simulator
  namespace: llm
  labels:
    app: vllm-simulator
    component: model-server
spec:
  predictor:
    # Use container runtime - no need for custom ServingRuntime since a sim
    containers:
    - name: kserve-container
      image: python:3.11-slim
      command: ["python", "-c"]
      args:
      - |
        import json
        import time
        from http.server import HTTPServer, BaseHTTPRequestHandler
        from urllib.parse import urlparse, parse_qs

        class OpenAIHandler(BaseHTTPRequestHandler):
            def do_GET(self):
                if self.path in ['/health', '/v1/models']:
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    if self.path == '/v1/models':
                        response = {"object": "list", "data": [{"id": "simulator-model", "object": "model"}]}
                    else:
                        response = {"status": "healthy"}
                    self.wfile.write(json.dumps(response).encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def do_POST(self):
                if self.path == '/v1/chat/completions':
                    content_length = int(self.headers['Content-Length'])
                    post_data = self.rfile.read(content_length)
                    try:
                        request = json.loads(post_data.decode())
                        response = {
                            "id": f"chatcmpl-{int(time.time())}",
                            "object": "chat.completion",
                            "created": int(time.time()),
                            "model": request.get("model", "simulator-model"),
                            "choices": [{
                                "index": 0,
                                "message": {
                                    "role": "assistant",
                                    "content": f"This is a simulated response to: {request['messages'][-1]['content']}"
                                },
                                "finish_reason": "stop"
                            }],
                            "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
                        }
                        self.send_response(200)
                        self.send_header('Content-Type', 'application/json')
                        self.end_headers()
                        self.wfile.write(json.dumps(response).encode())
                    except:
                        self.send_response(400)
                        self.end_headers()
                else:
                    self.send_response(404)
                    self.end_headers()

        print("Starting vLLM simulator on port 8080")
        server = HTTPServer(('0.0.0.0', 8080), OpenAIHandler)
        server.serve_forever()
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 256Mi
      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 5
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5